{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tensorflow\n",
        "%pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7uddHoZRu6I",
        "outputId": "603eec51-a551-4d07-d1fe-2142c240b1c8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hppd2g8Cj55m"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = pd.read_csv(\"data/tfidf.csv\")\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
        "\n",
        "# Initialize the NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "# Define a function to perform NER on a term\n",
        "def get_ner_tags(term):\n",
        "    ner_results = nlp(term)\n",
        "    tags = [entity['entity_group'] for entity in ner_results]\n",
        "    return \", \".join(tags)\n",
        "\n",
        "# Apply the NER function to each term\n",
        "data['NER Tags'] = data['Term'].apply(get_ner_tags)\n",
        "\n",
        "data.to_csv(\"data/tfidf_NER_largemodel.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "# Initialize the NER pipeline\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
        "\n",
        "# Define a function to perform NER on a term\n",
        "def get_ner_tags(term):\n",
        "    ner_results = nlp(term)\n",
        "    tags = [entity['entity_group'] for entity in ner_results]\n",
        "    return \", \".join(tags)\n",
        "\n",
        "# Apply the NER function to each term\n",
        "data['NER Tags'] = data['Term'].apply(get_ner_tags)\n",
        "\n",
        "data.to_csv(\"data/tfidf_NER1.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-large-ner-hrl\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-large-ner-hrl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract NER from OCR  directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('data\\ocr_texts_vip.csv')\n",
        "\n",
        "# Initialize the BERT NER model and tokenizer\n",
        "model_name = \"dslim/bert-large-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function to extract text between \"Subject:\"/\"Re:\" and \"CONFIDENTIAL\"\n",
        "def extract_relevant_text(text):\n",
        "    match = re.search(r'Subject:.*?CONFIDENTIAL|Re:.*?CONFIDENTIAL', text, re.DOTALL)\n",
        "    if match:\n",
        "        relevant_text = match.group(0)\n",
        "        # Remove 'CONFIDENTIAL' and the leading part up to \"Subject:\" or \"Re:\"\n",
        "        relevant_text = re.sub(r'(Subject:|Re:)', '', relevant_text)\n",
        "        relevant_text = relevant_text.replace('CONFIDENTIAL', '').strip()\n",
        "        return relevant_text\n",
        "    return \"\"\n",
        "\n",
        "# Function to extract NER tags\n",
        "def extract_ner_tags(text):\n",
        "    ner_results = ner_pipeline(text)\n",
        "    return ner_results\n",
        "\n",
        "# Process each OCR text and extract NER tags\n",
        "ner_data = []\n",
        "for index, row in df.iterrows():\n",
        "    doc_id = row['id']\n",
        "    text = extract_relevant_text(row['text'])\n",
        "    if text:\n",
        "        ner_results = extract_ner_tags(text)\n",
        "        for result in ner_results:\n",
        "            word = result['word']\n",
        "            entity = result['entity']\n",
        "            ner_data.append({'id': doc_id, 'word': word, 'tag': entity})\n",
        "            \n",
        "# Convert the results to a DataFrame\n",
        "ner_df = pd.DataFrame(ner_data)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_path = 'data/ner_output.csv'\n",
        "ner_df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the keyword extraction model and tokenizer\n",
        "model_name = \"transformer3/H2-keywordextractor\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for keyword extraction\n",
        "keyword_extraction_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to extract relevant text between \"Subject:\"/\"Re:\" and \"CONFIDENTIAL\"\n",
        "def extract_relevant_text(text):\n",
        "    match = re.search(r'(Subject:|Re:).*?(CONFIDENTIAL|$)', text, re.DOTALL)\n",
        "    if match:\n",
        "        relevant_text = match.group(0)\n",
        "        # Remove 'CONFIDENTIAL' and the leading part up to \"Subject:\" or \"Re:\"\n",
        "        relevant_text = re.sub(r'(Subject:|Re:)', '', relevant_text)\n",
        "        relevant_text = relevant_text.replace('CONFIDENTIAL', '').strip()\n",
        "        return relevant_text\n",
        "    return \"\"\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    keyword_results = keyword_extraction_pipeline(text)\n",
        "    return keyword_results\n",
        "\n",
        "# Process each OCR text and extract keywords\n",
        "keyword_data = []\n",
        "for index, row in df.iterrows():\n",
        "    doc_id = row['id']\n",
        "    text = extract_relevant_text(row['text'])\n",
        "    if text:\n",
        "        keyword_results = extract_keywords(text)\n",
        "        for result in keyword_results:\n",
        "            word = result['word']\n",
        "            entity = result['entity']\n",
        "            keyword_data.append({'id': doc_id, 'word': word, 'tag': entity})\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "keyword_df = pd.DataFrame(keyword_data)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "output_path = 'data/keyword_output.csv'\n",
        "keyword_df.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "data = {\n",
        "    'TD': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'text': [\n",
        "        # ... (your text data here)\n",
        "    ]\n",
        "}\n",
        "dataset = Dataset.from_dict(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
