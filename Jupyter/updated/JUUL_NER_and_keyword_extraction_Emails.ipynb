{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition (NER) and Keyword Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgJDNNa4Geak"
      },
      "source": [
        "This notebook coves the topic of identifying which celebrities JUUL targeted to promote their vaping product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Onca6x_RBm"
      },
      "source": [
        "For researchers who would prefer to work with optical character recognition (OCR) text for JUUL vs State of North Carolina case from within their own database systems, these files are available for free download via the link below.\n",
        "https://ucsf.app.box.com/v/IDL-DataSets/file/1447029625798\n",
        "\n",
        "Note: The link provides access to the most current dataset, as the website undergoes a new release each month. Ensure that you have sufficient storage available to download the zip file with the OCR text (~32GB).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-DbFE4ED0V5"
      },
      "source": [
        "#### Step 1: Retrieve documents ids relevant to the query using the API wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5bjMeddHIXu"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TeBnQn4XG_JX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import polars as pl\n",
        "from industryDocumentsWrapper import IndustryDocsSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUec9AQE_EPZ"
      },
      "source": [
        "Code to retrieve document ids which will then be used to retrieve OCR text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZW1mkc8I_Beh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100/11052 documents collected\n",
            "200/11052 documents collected\n",
            "300/11052 documents collected\n",
            "400/11052 documents collected\n",
            "500/11052 documents collected\n",
            "600/11052 documents collected\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(collection:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJUUL labs Collection\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND case:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState of North Carolina\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AND Youth OR adolescent AND type:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmail\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# modify as required\u001b[39;00m\n\u001b[1;32m      2\u001b[0m api \u001b[38;5;241m=\u001b[39m IndustryDocsSearch()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m api\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcelebrity_email_query.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/industryDocumentsWrapper/ucsf_api.py:121\u001b[0m, in \u001b[0;36mIndustryDocsSearch.query\u001b[0;34m(self, q, case, collection, type, industry, brand, availability, date, id, author, source, bates, box, originalformat, wt, cursor_mark, sort, n)\u001b[0m\n\u001b[1;32m    118\u001b[0m     industry \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=industry:)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+(?=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, query)\u001b[38;5;241m.\u001b[39mgroup()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Queries the UCSF Industry Documents Solr Library for documents\"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loop_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m industry:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_links(industry)\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/industryDocumentsWrapper/ucsf_api.py:55\u001b[0m, in \u001b[0;36mIndustryDocsSearch._loop_results\u001b[0;34m(self, query, n)\u001b[0m\n\u001b[1;32m     52\u001b[0m     current_cursor \u001b[38;5;241m=\u001b[39m next_cursor\n\u001b[1;32m     53\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cursormark(query, current_cursor)\n\u001b[0;32m---> 55\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     56\u001b[0m docs \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs):\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "query = '(collection:\"JUUL labs Collection\" AND case:\"State of North Carolina\" AND Youth OR adolescent AND type:\"Email\")' # modify as required\n",
        "api = IndustryDocsSearch()\n",
        "api.query(q=query, n=-1)\n",
        "api.save('celebrity_email_query.parquet', format='parquet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "celebrity_emails = pl.read_parquet('celebrity_email_query.parquet')\n",
        "nc_emails = pl.read_parquet('../../data/juul_nc_emails.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original nc_emails shape: (1685701, 71)\n",
            "Celebrity emails shape: (34091, 25)\n",
            "Filtered nc_emails shape: (34090, 71)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>bates</th><th>type</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;hqmp0299&quot;</td><td>&quot;JLI05753504&quot;</td><td>&quot;email&quot;</td></tr><tr><td>&quot;gypy0299&quot;</td><td>&quot;JLI04787118&quot;</td><td>&quot;email&quot;</td></tr><tr><td>&quot;xypy0299&quot;</td><td>&quot;JLI04787139&quot;</td><td>&quot;email&quot;</td></tr><tr><td>&quot;ssnx0338&quot;</td><td>&quot;JLI42903959&quot;</td><td>&quot;email&quot;</td></tr><tr><td>&quot;nnyx0338&quot;</td><td>&quot;JLI00448237&quot;</td><td>&quot;email&quot;</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 3)\n",
              "┌──────────┬─────────────┬───────┐\n",
              "│ id       ┆ bates       ┆ type  │\n",
              "│ ---      ┆ ---         ┆ ---   │\n",
              "│ str      ┆ str         ┆ str   │\n",
              "╞══════════╪═════════════╪═══════╡\n",
              "│ hqmp0299 ┆ JLI05753504 ┆ email │\n",
              "│ gypy0299 ┆ JLI04787118 ┆ email │\n",
              "│ xypy0299 ┆ JLI04787139 ┆ email │\n",
              "│ ssnx0338 ┆ JLI42903959 ┆ email │\n",
              "│ nnyx0338 ┆ JLI00448237 ┆ email │\n",
              "└──────────┴─────────────┴───────┘"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Filter the nc_emails DataFrame to only include entries whose 'id' is present in celebrity_emails\n",
        "filtered_nc_emails = nc_emails.filter(pl.col(\"id\").is_in(celebrity_emails[\"id\"]))\n",
        "\n",
        "# Display the shape of the original and filtered DataFrames to see how many emails match\n",
        "print(f\"Original nc_emails shape: {nc_emails.shape}\")\n",
        "print(f\"Celebrity emails shape: {celebrity_emails.shape}\")\n",
        "print(f\"Filtered nc_emails shape: {filtered_nc_emails.shape}\")\n",
        "\n",
        "# Preview the filtered DataFrame\n",
        "filtered_nc_emails.select([\"id\", \"bates\", \"type\"]).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_nc_emails.write_parquet('celebrity_emails_ocr_text.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8JWKd6PxUvQ"
      },
      "source": [
        "#### Named Entity Recognition (NER) is a method that identifies and classifies key information within the text into predefined categories such as names of people, organizations, locations, dates, and other entities.\n",
        "\n",
        "#### For instance, NER can be used to identify key celebrities and organizations within the legal documents. This can be achieved using BERT (Bidirectional Encoder Representations from Transformers) which is a model designed to help machines understand human language more effectively. It's based on a type of neural network architecture called a transformer, which is particularly good at processing sequences of data, like sentences.\n",
        "\n",
        "#### Transformer-based models can be fine-tuned for specific tasks by training them on specialized datasets. The model's ability to recognize different categories, such as sentiment, named entities, or topics, depends on what data it was trained on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrmUPfsRu8cd"
      },
      "source": [
        "Run the code cell below to perform name entity recognition analysis directly using the OCR text (email). The output is seperated by each individual tag for ease of analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pl.read_parquet('celebrity_emails_ocr_text.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Let's look at some of the columns that we're working with**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['id',\n",
              " 'tid',\n",
              " 'bates',\n",
              " 'type',\n",
              " 'description',\n",
              " 'title',\n",
              " 'author',\n",
              " 'mentioned',\n",
              " 'attending',\n",
              " 'copied',\n",
              " 'recipient',\n",
              " 'redacted',\n",
              " 'collection_name',\n",
              " 'pages',\n",
              " 'exhibit_number',\n",
              " 'document_date',\n",
              " 'date_added_ucsf',\n",
              " 'date_modified_ucsf',\n",
              " 'date_added_industry',\n",
              " 'date_modified_industry',\n",
              " 'date_produced',\n",
              " 'date_shipped',\n",
              " 'deposition_date',\n",
              " 'date_privilege_logged',\n",
              " 'case',\n",
              " 'industry',\n",
              " 'drug',\n",
              " 'adverse_ruling',\n",
              " 'area',\n",
              " 'bates_alternate',\n",
              " 'box',\n",
              " 'brand',\n",
              " 'country',\n",
              " 'language',\n",
              " 'court',\n",
              " 'format',\n",
              " 'express_waiver',\n",
              " 'file',\n",
              " 'genre',\n",
              " 'keywords',\n",
              " 'bates_master',\n",
              " 'other_number',\n",
              " 'request_number',\n",
              " 'minnesota_request_number',\n",
              " 'privilege_code',\n",
              " 'topic',\n",
              " 'witness',\n",
              " 'cited',\n",
              " 'availability',\n",
              " 'grant_number',\n",
              " 'source',\n",
              " 'folder',\n",
              " 'series',\n",
              " 'chemical',\n",
              " 'food',\n",
              " 'rights',\n",
              " 'attachment',\n",
              " 'attachmentnum',\n",
              " 'conversation',\n",
              " 'conversationid',\n",
              " 'custodian',\n",
              " 'datereceived',\n",
              " 'datesent',\n",
              " 'filename',\n",
              " 'filepath',\n",
              " 'messageid',\n",
              " 'subject',\n",
              " 'timereceived',\n",
              " 'timesent',\n",
              " 'redaction',\n",
              " 'ocr_text']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Let's slim down our dataframe to just some columns that may be relevant**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "emails_df = df.select([\"id\", \"bates\", \"type\", \"document_date\", \"title\", \"author\", \"mentioned\", \"copied\", \"recipient\", \"ocr_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>bates</th><th>type</th><th>document_date</th><th>title</th><th>author</th><th>mentioned</th><th>copied</th><th>recipient</th><th>ocr_text</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;sgyk0288&quot;</td><td>&quot;JLI06795128&quot;</td><td>&quot;email&quot;</td><td>&quot;Tue Jan 16 16:00:00 PST 2018&quot;</td><td>&quot;Re: These are the stories that…</td><td>&quot;Ben Schwartz &lt;bschwartz@gacapi…</td><td>&quot; &quot;</td><td>&quot;Jenny Kim &lt;&quot;jenny kim &lt;jenny@j…</td><td>&quot;Vittal Kadapakkam &lt;&quot;vittal kad…</td><td>&quot;From:&nbsp;&nbsp;To:&nbsp;&nbsp;CC:&nbsp;&nbsp;Sent:&nbsp;&nbsp;Subjec…</td></tr><tr><td>&quot;qhxb0302&quot;</td><td>&quot;JLI05118491&quot;</td><td>&quot;email&quot;</td><td>&quot;Mon Dec 03 16:00:00 PST 2018&quot;</td><td>&quot;Re:L Redacted&quot;</td><td>&quot;Redacted&quot;</td><td>&quot; &quot;</td><td>&quot; &quot;</td><td>&quot;Jessica Taylor&quot;</td><td>&quot;From:&nbsp;&nbsp;To:&nbsp;&nbsp;Sent:&nbsp;&nbsp;Subject:&nbsp;&nbsp;&nbsp;…</td></tr><tr><td>&quot;hllp0316&quot;</td><td>&quot;JLI04498841&quot;</td><td>&quot;email&quot;</td><td>&quot;Mon Apr 13 17:00:00 PDT 2015&quot;</td><td>&quot;Re: Pax 2 Same Day Delivery Se…</td><td>&quot;Paul Moraes &lt;paul@ploom.com&gt;&quot;</td><td>&quot; &quot;</td><td>&quot;Laina Payne &lt;&quot;laina payne &lt;lai…</td><td>&quot;Rafael Burde &lt;&quot;rafael burde &lt;r…</td><td>&quot;From:&nbsp;&nbsp;To:&nbsp;&nbsp;CC:&nbsp;&nbsp;&nbsp;&nbsp;Paul Moraes…</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (3, 10)\n",
              "┌──────────┬────────────┬───────┬────────────┬───┬───────────┬────────────┬────────────┬───────────┐\n",
              "│ id       ┆ bates      ┆ type  ┆ document_d ┆ … ┆ mentioned ┆ copied     ┆ recipient  ┆ ocr_text  │\n",
              "│ ---      ┆ ---        ┆ ---   ┆ ate        ┆   ┆ ---       ┆ ---        ┆ ---        ┆ ---       │\n",
              "│ str      ┆ str        ┆ str   ┆ ---        ┆   ┆ str       ┆ str        ┆ str        ┆ str       │\n",
              "│          ┆            ┆       ┆ str        ┆   ┆           ┆            ┆            ┆           │\n",
              "╞══════════╪════════════╪═══════╪════════════╪═══╪═══════════╪════════════╪════════════╪═══════════╡\n",
              "│ sgyk0288 ┆ JLI0679512 ┆ email ┆ Tue Jan 16 ┆ … ┆           ┆ Jenny Kim  ┆ Vittal     ┆ From:     │\n",
              "│          ┆ 8          ┆       ┆ 16:00:00   ┆   ┆           ┆ <\"jenny    ┆ Kadapakkam ┆ To:  CC:  │\n",
              "│          ┆            ┆       ┆ PST 2018   ┆   ┆           ┆ kim        ┆ <\"vittal   ┆ Sent:     │\n",
              "│          ┆            ┆       ┆            ┆   ┆           ┆ <jenny@j…  ┆ kad…       ┆ Subjec…   │\n",
              "│ qhxb0302 ┆ JLI0511849 ┆ email ┆ Mon Dec 03 ┆ … ┆           ┆            ┆ Jessica    ┆ From:     │\n",
              "│          ┆ 1          ┆       ┆ 16:00:00   ┆   ┆           ┆            ┆ Taylor     ┆ To:       │\n",
              "│          ┆            ┆       ┆ PST 2018   ┆   ┆           ┆            ┆            ┆ Sent:     │\n",
              "│          ┆            ┆       ┆            ┆   ┆           ┆            ┆            ┆ Subject:  │\n",
              "│          ┆            ┆       ┆            ┆   ┆           ┆            ┆            ┆ …         │\n",
              "│ hllp0316 ┆ JLI0449884 ┆ email ┆ Mon Apr 13 ┆ … ┆           ┆ Laina      ┆ Rafael     ┆ From:     │\n",
              "│          ┆ 1          ┆       ┆ 17:00:00   ┆   ┆           ┆ Payne      ┆ Burde      ┆ To:  CC:  │\n",
              "│          ┆            ┆       ┆ PDT 2015   ┆   ┆           ┆ <\"laina    ┆ <\"rafael   ┆ Paul      │\n",
              "│          ┆            ┆       ┆            ┆   ┆           ┆ payne      ┆ burde <r…  ┆ Moraes…   │\n",
              "│          ┆            ┆       ┆            ┆   ┆           ┆ <lai…      ┆            ┆           │\n",
              "└──────────┴────────────┴───────┴────────────┴───┴───────────┴────────────┴────────────┴───────────┘"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emails_df.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Now, we'll go ahead and import the necessary Python libraries and set up our model for NER**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rolando/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, ModernBertConfig, pipeline\n",
        "from itertools import tee, islice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**[bert-base-NER](https://huggingface.co/dslim/bert-base-NER) is a model that is built on the [base-BERT](https://huggingface.co/google-bert/bert-base-uncased) large language model and has been specifically fine-tuned for NER tasks.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "# Initialize the BERT NER model and tokenizer\n",
        "model_name = \"dslim/bert-base-NER\"\n",
        "# config = ModernBertConfig.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the pipeline\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_relevant_text(text):\n",
        "    match = re.search(r'Subject:.*?CONFIDENTIAL|Re:.*?CONFIDENTIAL', text, re.DOTALL)\n",
        "    if match:\n",
        "        relevant_text = match.group(0)\n",
        "        # Remove 'CONFIDENTIAL' and the leading part up to \"Subject:\" or \"Re:\"\n",
        "        relevant_text = re.sub(r'(Subject:|Re:)', '', relevant_text)\n",
        "        relevant_text = relevant_text.replace('CONFIDENTIAL', '').strip()\n",
        "        return relevant_text\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of emails_df with cleaned text: (34090, 11)\n",
            "Sample of cleaned text:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>bates</th><th>cleaned_text</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;hqmp0299&quot;</td><td>&quot;JLI05753504&quot;</td><td>&quot;Kate Morgan on behalf of Kate …</td></tr><tr><td>&quot;gypy0299&quot;</td><td>&quot;JLI04787118&quot;</td><td>&quot;Jessica Edmondson&nbsp;&nbsp;3/2/2018 10…</td></tr><tr><td>&quot;xypy0299&quot;</td><td>&quot;JLI04787139&quot;</td><td>&quot;Jessica Edmondson on behalf of…</td></tr><tr><td>&quot;ssnx0338&quot;</td><td>&quot;JLI42903959&quot;</td><td>&quot;Michael Swanson on behalf of M…</td></tr><tr><td>&quot;nnyx0338&quot;</td><td>&quot;JLI00448237&quot;</td><td>&quot;Kate Morgan on behalf of Kate …</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 3)\n",
              "┌──────────┬─────────────┬─────────────────────────────────┐\n",
              "│ id       ┆ bates       ┆ cleaned_text                    │\n",
              "│ ---      ┆ ---         ┆ ---                             │\n",
              "│ str      ┆ str         ┆ str                             │\n",
              "╞══════════╪═════════════╪═════════════════════════════════╡\n",
              "│ hqmp0299 ┆ JLI05753504 ┆ Kate Morgan on behalf of Kate … │\n",
              "│ gypy0299 ┆ JLI04787118 ┆ Jessica Edmondson  3/2/2018 10… │\n",
              "│ xypy0299 ┆ JLI04787139 ┆ Jessica Edmondson on behalf of… │\n",
              "│ ssnx0338 ┆ JLI42903959 ┆ Michael Swanson on behalf of M… │\n",
              "│ nnyx0338 ┆ JLI00448237 ┆ Kate Morgan on behalf of Kate … │\n",
              "└──────────┴─────────────┴─────────────────────────────────┘"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply the extract_relevant_text function to the ocr_text column and create a new column\n",
        "emails_df = emails_df.with_columns(\n",
        "    pl.col(\"ocr_text\").map_elements(extract_relevant_text, return_dtype=pl.String).alias(\"cleaned_text\")\n",
        ")\n",
        "\n",
        "# Display the first few rows to see the results\n",
        "print(f\"Shape of emails_df with cleaned text: {emails_df.shape}\")\n",
        "print(\"Sample of cleaned text:\")\n",
        "emails_df.select([\"id\", \"bates\", \"cleaned_text\"]).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to extract NER tags\n",
        "def extract_ner_tags(text):\n",
        "    ner_results = ner_pipeline(text)\n",
        "    return ner_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Let's take a look at what the model pulled from the first email** \n",
        "\n",
        "To see what the NER entities refer to, see the [bert-base-NER description](https://huggingface.co/dslim/bert-base-NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'B-PER',\n",
              "  'score': 0.9996094,\n",
              "  'index': 1,\n",
              "  'word': 'Jessica',\n",
              "  'start': 0,\n",
              "  'end': 7},\n",
              " {'entity': 'I-PER',\n",
              "  'score': 0.9990388,\n",
              "  'index': 2,\n",
              "  'word': 'Edmond',\n",
              "  'start': 8,\n",
              "  'end': 14},\n",
              " {'entity': 'I-ORG',\n",
              "  'score': 0.39536187,\n",
              "  'index': 41,\n",
              "  'word': 'On',\n",
              "  'start': 124,\n",
              "  'end': 126},\n",
              " {'entity': 'I-MISC',\n",
              "  'score': 0.785701,\n",
              "  'index': 42,\n",
              "  'word': 'Fr',\n",
              "  'start': 127,\n",
              "  'end': 129},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': 0.33906993,\n",
              "  'index': 43,\n",
              "  'word': '##i',\n",
              "  'start': 129,\n",
              "  'end': 130},\n",
              " {'entity': 'B-PER',\n",
              "  'score': 0.9995409,\n",
              "  'index': 55,\n",
              "  'word': 'Jessica',\n",
              "  'start': 157,\n",
              "  'end': 164},\n",
              " {'entity': 'I-PER',\n",
              "  'score': 0.9978004,\n",
              "  'index': 56,\n",
              "  'word': 'Edmond',\n",
              "  'start': 165,\n",
              "  'end': 171},\n",
              " {'entity': 'B-PER',\n",
              "  'score': 0.99941754,\n",
              "  'index': 221,\n",
              "  'word': 'Jessica',\n",
              "  'start': 708,\n",
              "  'end': 715},\n",
              " {'entity': 'I-PER',\n",
              "  'score': 0.9981589,\n",
              "  'index': 222,\n",
              "  'word': 'Edmond',\n",
              "  'start': 716,\n",
              "  'end': 722},\n",
              " {'entity': 'B-LOC',\n",
              "  'score': 0.99800915,\n",
              "  'index': 253,\n",
              "  'word': 'NYC',\n",
              "  'start': 787,\n",
              "  'end': 790},\n",
              " {'entity': 'B-PER',\n",
              "  'score': 0.9859094,\n",
              "  'index': 264,\n",
              "  'word': 'Avery',\n",
              "  'start': 831,\n",
              "  'end': 836},\n",
              " {'entity': 'B-LOC',\n",
              "  'score': 0.43442997,\n",
              "  'index': 275,\n",
              "  'word': 'Tu',\n",
              "  'start': 880,\n",
              "  'end': 882},\n",
              " {'entity': 'B-PER',\n",
              "  'score': 0.99294245,\n",
              "  'index': 309,\n",
              "  'word': 'Avery',\n",
              "  'start': 999,\n",
              "  'end': 1004}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_ner_tags(emails_df['cleaned_text'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ComputeError",
          "evalue": "KeyboardInterrupt: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add NER data to the emails_df dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m emails_df \u001b[38;5;241m=\u001b[39m \u001b[43memails_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_ner_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mList\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner_entities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Let's check a sample to see the results\u001b[39;00m\n\u001b[1;32m      7\u001b[0m emails_df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_entities\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m5\u001b[39m)\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/polars/dataframe/frame.py:9805\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   9659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[1;32m   9660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9661\u001b[0m     \u001b[38;5;241m*\u001b[39mexprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr],\n\u001b[1;32m   9662\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[1;32m   9663\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9665\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[1;32m   9666\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9803\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴─────────────┘\u001b[39;00m\n\u001b[1;32m   9804\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 9805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/polars/lazyframe/frame.py:2065\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, _type_check, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, streaming, engine, background, _check_order, _eager, **_kwargs)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[0;32m-> 2065\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mComputeError\u001b[0m: KeyboardInterrupt: "
          ]
        }
      ],
      "source": [
        "# Add NER data to the emails_df dataframe\n",
        "ner_df = emails_df.with_columns(\n",
        "    pl.col(\"cleaned_text\").map_elements(lambda text: extract_ner_tags(text) if text else [], return_dtype=pl.List).alias(\"ner_entities\")\n",
        ")\n",
        "\n",
        "# Let's check a sample to see the results\n",
        "ner_df.select([\"id\", \"bates\", \"cleaned_text\", \"ner_entities\"]).sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Let's go ahead and save this new dataframe to a parquet file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner_df.write_parquet('celebrity_emails_ner.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>word</th><th>tag</th></tr><tr><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;ffbf0286&quot;</td><td>&quot;Kevin&quot;</td><td>&quot;LABEL_1&quot;</td></tr><tr><td>&quot;ffbf0286&quot;</td><td>&quot; &lt;kburns©ju&quot;</td><td>&quot;LABEL_1&quot;</td></tr><tr><td>&quot;ffbf0286&quot;</td><td>&quot;.&quot;</td><td>&quot;LABEL_1&quot;</td></tr><tr><td>&quot;ffbf0286&quot;</td><td>&quot;&gt;&nbsp;&nbsp;&quot;</td><td>&quot;LABEL_1&quot;</td></tr><tr><td>&quot;ffbf0286&quot;</td><td>&quot; Morgan&nbsp;&nbsp;Ashley&quot;</td><td>&quot;LABEL_1&quot;</td></tr></tbody></table></div>"
            ],
            "text/plain": [
              "shape: (5, 3)\n",
              "┌──────────┬─────────────────┬─────────┐\n",
              "│ id       ┆ word            ┆ tag     │\n",
              "│ ---      ┆ ---             ┆ ---     │\n",
              "│ str      ┆ str             ┆ str     │\n",
              "╞══════════╪═════════════════╪═════════╡\n",
              "│ ffbf0286 ┆ Kevin           ┆ LABEL_1 │\n",
              "│ ffbf0286 ┆  <kburns©ju     ┆ LABEL_1 │\n",
              "│ ffbf0286 ┆ .               ┆ LABEL_1 │\n",
              "│ ffbf0286 ┆ >               ┆ LABEL_1 │\n",
              "│ ffbf0286 ┆  Morgan  Ashley ┆ LABEL_1 │\n",
              "└──────────┴─────────────────┴─────────┘"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "category_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivMPmNIo6MZz"
      },
      "source": [
        "Run the code cell below to remove duplicates in the word column for the person file. This is useful in cases where you you want a more concise list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TEb-O5tR6Rqb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(36, 3)\n",
            "(10, 3)\n",
            "(18, 3)\n",
            "(61, 3)\n"
          ]
        }
      ],
      "source": [
        "# Remove duplicates in ner_output_PER based on word column\n",
        "# Load the parquet file\n",
        "for category in categories:\n",
        "    df = pl.read_parquet(f'ner_output_{category}.parquet')\n",
        "\n",
        "    # Remove duplicates based on the 'word' column\n",
        "    df_cleaned = df.unique(pl.col('word'))\n",
        "    print(df_cleaned.shape)\n",
        "\n",
        "    # Save the de-duplicated DataFrame to a new Parquet file\n",
        "    df_cleaned.write_parquet(f'ner_output_{category}_cleaned.parquet',)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7JJp11_1uNX"
      },
      "source": [
        "**Similarly, the another model can be used to extract keywords from the OCR text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YnyUOCcE1402"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "'ocr_content' is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miter_rows():\n\u001b[1;32m     34\u001b[0m     doc_id \u001b[38;5;241m=\u001b[39m row[df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m---> 35\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_relevant_text(\u001b[38;5;28mstr\u001b[39m(row[\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mocr_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m]))\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[0;31mValueError\u001b[0m: 'ocr_content' is not in list"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "# Load the parquet file\n",
        "df = pl.read_parquet('celebrity_emails_ocr_text.parquet')\n",
        "\n",
        "# Initialize the keyword extraction model and tokenizer\n",
        "model_name = \"transformer3/H2-keywordextractor\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for keyword extraction\n",
        "keyword_extraction_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to extract text between \"Subject:\"/\"Re:\" and \"CONFIDENTIAL\".\n",
        "# This function searches for text between \"Subject:\" or \"Re:\" and \"CONFIDENTIAL\" within a given text to extract content from the email body.\n",
        "def extract_relevant_text(text):\n",
        "    match = re.search(r'(Subject:|Re:).*?(CONFIDENTIAL|$)', text, re.DOTALL)\n",
        "    if match:\n",
        "        relevant_text = match.group(0)\n",
        "        # Remove 'CONFIDENTIAL' and the leading part up to \"Subject:\" or \"Re:\"\n",
        "        relevant_text = re.sub(r'(Subject:|Re:)', '', relevant_text)\n",
        "        relevant_text = relevant_text.replace('CONFIDENTIAL', '').strip()\n",
        "        return relevant_text\n",
        "    return \"\"\n",
        "\n",
        "# Function to extract keywords\n",
        "def extract_keywords(text):\n",
        "    keyword_results = keyword_extraction_pipeline(text)\n",
        "    return keyword_results\n",
        "\n",
        "# Process each OCR text and extract keywords\n",
        "keyword_data = []\n",
        "for row in df.iter_rows():\n",
        "    doc_id = row[df.columns.index('id')]\n",
        "    text = extract_relevant_text(str(row[df.columns.index('ocr_content')]))\n",
        "    if text:\n",
        "        try:\n",
        "            keyword_results = extract_keywords(text)\n",
        "            for result in keyword_results:\n",
        "                keywords = result['generated_text'].split(\", \")\n",
        "                for keyword in keywords:\n",
        "                    keyword_data.append({'id': doc_id, 'keyword': keyword})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "keyword_df = pl.DataFrame(keyword_data)\n",
        "print(keyword_df)\n",
        "# Save the results to a new Parquet file\n",
        "output_path = 'keyword_output.parquet'\n",
        "keyword_df.write_parquet(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoConfig, pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "# Load the parquet file\n",
        "df = pl.read_parquet('juul_query_with_ocr.parquet')\n",
        "\n",
        "# Initialize the keyword extraction model and tokenizer\n",
        "model_name = \"google-t5/t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_config(config)\n",
        "\n",
        "# Create a pipeline for keyword extraction\n",
        "keyword_extraction_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_relevant_text(text):\n",
        "    match = re.search(r'(Subject:|Re:).*?(CONFIDENTIAL|$)', text, re.DOTALL)\n",
        "    if match:\n",
        "        relevant_text = match.group(0)\n",
        "        # Remove 'CONFIDENTIAL' and the leading part up to \"Subject:\" or \"Re:\"\n",
        "        relevant_text = re.sub(r'(Subject:|Re:)', '', relevant_text)\n",
        "        relevant_text = relevant_text.replace('CONFIDENTIAL', '').strip()\n",
        "        return relevant_text\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_keywords(text):\n",
        "    keyword_results = keyword_extraction_pipeline(text)\n",
        "    return keyword_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each OCR text and extract keywords\n",
        "keyword_data = []\n",
        "for row in df.iter_rows():\n",
        "    doc_id = row[df.columns.index('id')]\n",
        "    text = extract_relevant_text(str(row[df.columns.index('ocr_text')]))\n",
        "    if text:\n",
        "        try:\n",
        "            keyword_results = extract_keywords(text)\n",
        "            for result in keyword_results:\n",
        "                keywords = result['generated_text'].split(\", \")\n",
        "                for keyword in keywords:\n",
        "                    keyword_data.append({'id': doc_id, 'keyword': keyword})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "keyword_df = pl.DataFrame(keyword_data)\n",
        "keyword_df.sample(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
