{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating document-type datasets from the JUUL Labs Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we will walk through how to create a dataset of all documents of a particular type from the UNC case within the JUUL Labs Collection. This workflow can be adjusted to work with other document types, documents from other cases, or documents from other collections from the Industry Documents Library.\n",
        "\n",
        "The driving reason for this tutorial is that the [zipped folder for the JUUL Labs Collection](https://ucsf.app.box.com/v/IDL-DataSets/folder/78644252849) is almost 32 gigabytes alone, and much larger if you unzip the folder! This can take up a lot of space on your computer, especially if you're also creating derivative datasets.\n",
        "\n",
        "We recommend downloading the zipped folder and running this code in a virtual machine or cloud computing environment, then downloading the created dataset to your own computer for processing. If you can do all of your analysis in a virtual environment, all the better!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xvk1f05vIlF7"
      },
      "outputs": [],
      "source": [
        "# Let's import the necessary libraries for this tutorial\n",
        "import os\n",
        "import zipfile\n",
        "import polars as pl\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import threading\n",
        "from tqdm import tqdm  # For progress tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) We'll specify our zipped folder's path and where we want to save our created file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BzB5_vKRW2FJ"
      },
      "outputs": [],
      "source": [
        "zip_path = 'JUUL_Labs_Collection.zip'\n",
        "save_dir = '/content/unc-emails'\n",
        "last_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_content(zip_dir, save_dir, start_index=0):\n",
        "    # Create save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Shared dictionary to store the combined dataframe\n",
        "    main_df_dict = {}\n",
        "    lock = threading.Lock()\n",
        "\n",
        "    # Get list of files to process\n",
        "    with zipfile.ZipFile(zip_dir, 'r') as zipf:\n",
        "        files_to_process = [(file) for idx, file in enumerate(zipf.namelist()) \n",
        "                           if idx >= start_index and file.endswith('.csv')]\n",
        "\n",
        "    total_files = len(files_to_process)\n",
        "    print(f\"Starting to process {total_files} CSV files with 6 workers (from index {start_index})...\")\n",
        "\n",
        "    # Process files in parallel\n",
        "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
        "        # Using tqdm to show progress\n",
        "        futures = []\n",
        "        for file in files_to_process:\n",
        "            future = executor.submit(file, zip_dir, save_dir, file, lock, main_df_dict)\n",
        "            futures.append(future)\n",
        "\n",
        "        # Track progress\n",
        "        for future in tqdm(futures, total=total_files, desc=\"Processing files\"):\n",
        "            result = future.result()\n",
        "    \n",
        "    # Save the combined dataframe\n",
        "    if 'df' in main_df_dict and main_df_dict['df'] is not None:\n",
        "        output_file = os.path.join(save_dir, 'juul_nc_emails.parquet')\n",
        "        main_df_dict['df'].write_parquet(output_file)\n",
        "        print(f\"Saved combined dataframe with {main_df_dict['df'].shape[0]} rows to {output_file}\")\n",
        "    else:\n",
        "        print(\"No data was found to save\")\n",
        "\n",
        "    print(f\"Completed processing {total_files} files starting from index {start_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(zip_dir, save_dir, file, lock, main_df_dict):\n",
        "    # Only process CSV files\n",
        "    if file.endswith('.csv'):\n",
        "        with zipfile.ZipFile(zip_dir, 'r') as zipf:\n",
        "            with zipf.open(file) as f:\n",
        "                try:\n",
        "                    # filters for 'State of North Carolina' and 'email' (this can be adjusted for your project)\n",
        "                    temp_df = pl.read_csv(f, separator='|')\\\n",
        "                        .filter((pl.col('case').str.contains('State of North Carolina')),\n",
        "                                (pl.col('type').str.contains('email')))\n",
        "\n",
        "                    if temp_df.shape[0] != 0:\n",
        "                        with lock:\n",
        "                            if 'df' not in main_df_dict:\n",
        "                                main_df_dict['df'] = temp_df\n",
        "                            else:\n",
        "                                main_df_dict['df'] = pl.concat([main_df_dict['df'], temp_df])\n",
        "                        return f'Successfully processed file and added to main dataframe'\n",
        "                    return f'Empty result for file'\n",
        "                except Exception as e:\n",
        "                    return f'Error processing file: {str(e)}'\n",
        "    return f'Skipped non-CSV file'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZPPrvBcNlN0",
        "outputId": "3f726438-697e-488f-c8b8-7bcf2a02cba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting to process 2206 files with 6 workers (from index 0)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 2206/2206 [11:56<00:00,  3.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed processing 2206 files starting from index 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "save_content(zip_path, save_dir, last_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4s60wbBZAZD"
      },
      "outputs": [],
      "source": [
        "save_content(zip_path, save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYNuhw93coOf",
        "outputId": "2884afcf-5b5f-43b8-af91-6d3e685e48d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows: 1685701\n"
          ]
        }
      ],
      "source": [
        "# Now, let's read the saved parquet file to verify the results\n",
        "# This is a simple check to see if the file was saved correctly by verifying the number of rows\n",
        "import polars as pl\n",
        "\n",
        "df = pl.read_parquet('juul_unc_emails.parquet')\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
